{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2: Multi-Class Disease Classification\n",
    "\n",
    "This pipeline creates a dataset for disease classification (healthy excluded).\n",
    "\n",
    "Goal: Balance all disease classes to ~1000 samples each for better training.\n",
    "\n",
    "Note: This model should only run on samples classified as 'disease' by Pipeline 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Find project root\n",
    "PROJECT_ROOT = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "\n",
    "def make_absolute(path_str):\n",
    "    \"\"\"Convert relative path from .env to absolute path.\"\"\"\n",
    "    path = Path(path_str)\n",
    "    return path.resolve() if path.is_absolute() else (PROJECT_ROOT / path).resolve()\n",
    "\n",
    "# Dataset paths\n",
    "TRAIN_LABELS_CSV = make_absolute(os.getenv('TRAIN_LABELS_CSV'))\n",
    "TEST_LABELS_CSV = make_absolute(os.getenv('TEST_LABELS_CSV'))\n",
    "TRAIN_IMAGES_DIR = make_absolute(os.getenv('TRAIN_IMAGES_DIR'))\n",
    "TEST_IMAGES_DIR = make_absolute(os.getenv('TEST_IMAGES_DIR'))\n",
    "\n",
    "# Output paths for DISEASE classification\n",
    "OUTPUT_DISEASE_BASE_DIR = PROJECT_ROOT / 'dataset' / 'diseases'\n",
    "OUTPUT_DISEASE_IMAGES_TRAIN = OUTPUT_DISEASE_BASE_DIR / 'images' / 'train'\n",
    "OUTPUT_DISEASE_IMAGES_VAL = OUTPUT_DISEASE_BASE_DIR / 'images' / 'val'\n",
    "OUTPUT_DISEASE_LABELS_TRAIN = OUTPUT_DISEASE_BASE_DIR / 'labels' / 'train'\n",
    "OUTPUT_DISEASE_LABELS_VAL = OUTPUT_DISEASE_BASE_DIR / 'labels' / 'val'\n",
    "\n",
    "# Plant species\n",
    "PLANT_SPECIES = [s.strip() for s in os.getenv('PLANT_SPECIES').split(',')]\n",
    "\n",
    "# Configuration\n",
    "TARGET_SAMPLES_PER_CLASS = 1000\n",
    "\n",
    "print(\"‚úì Configuration loaded!\")\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n",
    "print(f\"Output: {OUTPUT_DISEASE_BASE_DIR}\")\n",
    "print(f\"Target samples per class: {TARGET_SAMPLES_PER_CLASS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv(TRAIN_LABELS_CSV)\n",
    "df_test = pd.read_csv(TEST_LABELS_CSV)\n",
    "\n",
    "print(f\"Loaded: {len(df_train)} train, {len(df_test)} test samples\")\n",
    "\n",
    "# Clean class names\n",
    "for df in [df_train, df_test]:\n",
    "    df['class'] = (\n",
    "        df['class']\n",
    "        .str.replace(r'(?i)leaf', '', regex=True)\n",
    "        .str.replace(r'\\s+', ' ', regex=True)\n",
    "        .str.replace(r'_', ' ', regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "print(\"‚úì Class names cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_species(text):\n",
    "    for plant in PLANT_SPECIES:\n",
    "        if re.search(rf\"\\b{plant}\\b\", text, flags=re.IGNORECASE):\n",
    "            return plant\n",
    "    return None\n",
    "\n",
    "def extract_disease(text):\n",
    "    for plant in PLANT_SPECIES:\n",
    "        text = re.sub(rf\"\\b{plant}\\b\", \"\", text, flags=re.IGNORECASE).strip()\n",
    "    return text if text else \"healthy\"\n",
    "\n",
    "# Extract features\n",
    "df_train['species'] = df_train['class'].apply(extract_species)\n",
    "df_train['disease'] = df_train['class'].apply(extract_disease)\n",
    "df_test['species'] = df_test['class'].apply(extract_species)\n",
    "df_test['disease'] = df_test['class'].apply(extract_disease)\n",
    "\n",
    "print(\"‚úì Features extracted\")\n",
    "print(df_train[['class', 'species', 'disease']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fix Zero Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_zero_dimensions(df, image_folder):\n",
    "    image_folder = Path(image_folder)\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['width'] == 0 or row['height'] == 0:\n",
    "            image_path = image_folder / row['filename']\n",
    "            if image_path.exists():\n",
    "                with Image.open(image_path) as img:\n",
    "                    w, h = img.size\n",
    "                    df.at[idx, 'width'] = w\n",
    "                    df.at[idx, 'height'] = h\n",
    "    return df\n",
    "\n",
    "df_train = fix_zero_dimensions(df_train, TRAIN_IMAGES_DIR)\n",
    "df_test = fix_zero_dimensions(df_test, TEST_IMAGES_DIR)\n",
    "print(\"‚úì Dimensions fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Files Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_files_exist(df, image_folder):\n",
    "    image_folder = Path(image_folder)\n",
    "    existing_mask = []\n",
    "    for _, row in df.iterrows():\n",
    "        existing_mask.append((image_folder / row['filename']).exists())\n",
    "    return df[existing_mask].copy()\n",
    "\n",
    "df_train = verify_files_exist(df_train, TRAIN_IMAGES_DIR)\n",
    "df_test = verify_files_exist(df_test, TEST_IMAGES_DIR)\n",
    "print(f\"‚úì Verified: {len(df_train)} train, {len(df_test)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Filter Out Healthy Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only disease samples (exclude healthy)\n",
    "df_diseases_only = df_train[df_train['disease'] != 'healthy'].copy()\n",
    "\n",
    "print(f\"Original training samples: {len(df_train)}\")\n",
    "print(f\"Disease samples only: {len(df_diseases_only)}\")\n",
    "print(f\"Removed healthy samples: {len(df_train) - len(df_diseases_only)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Remove Very Rare Diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extremely rare diseases (< 0.1% of dataset)\n",
    "disease_proportions = df_diseases_only['disease'].value_counts(normalize=True)\n",
    "rare_threshold = 0.001\n",
    "rare_diseases = disease_proportions[disease_proportions < rare_threshold].index.tolist()\n",
    "\n",
    "print(f\"Rare diseases (< {rare_threshold*100}%): {rare_diseases}\")\n",
    "\n",
    "df_diseases_clean = df_diseases_only[~df_diseases_only['disease'].isin(rare_diseases)].copy()\n",
    "\n",
    "print(f\"\\nAfter removing rare diseases: {len(df_diseases_clean)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Disease Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_counts = df_diseases_clean['disease'].value_counts()\n",
    "\n",
    "print(\"Disease distribution (before balancing):\")\n",
    "for disease, count in disease_counts.items():\n",
    "    print(f\"  {disease}: {count} samples\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x=disease_counts.index, y=disease_counts.values)\n",
    "plt.axhline(y=TARGET_SAMPLES_PER_CLASS, color='r', linestyle='--', label=f'Target: {TARGET_SAMPLES_PER_CLASS}')\n",
    "plt.title('Disease Distribution (Before Balancing)')\n",
    "plt.xlabel('Disease')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Balance Disease Classes via Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dfs = []\n",
    "\n",
    "for disease, group in df_diseases_clean.groupby(\"disease\"):\n",
    "    n_samples = len(group)\n",
    "    n_to_add = TARGET_SAMPLES_PER_CLASS - n_samples\n",
    "    \n",
    "    if n_to_add > 0:\n",
    "        print(f\"\\n{disease}: {n_samples} ‚Üí {TARGET_SAMPLES_PER_CLASS} (adding {n_to_add} duplicates)\")\n",
    "        \n",
    "        # Keep original samples\n",
    "        balanced_dfs.append(group)\n",
    "        \n",
    "        # Add duplicates with modified filenames\n",
    "        duplicates_added = 0\n",
    "        while duplicates_added < n_to_add:\n",
    "            # Cycle through samples\n",
    "            idx = duplicates_added % n_samples\n",
    "            sample = group.iloc[idx:idx+1].copy()\n",
    "            \n",
    "            # Modify filename to avoid conflicts\n",
    "            original_filename = sample['filename'].values[0]\n",
    "            stem = Path(original_filename).stem\n",
    "            suffix = Path(original_filename).suffix\n",
    "            new_filename = f\"{stem}_dup{duplicates_added}{suffix}\"\n",
    "            sample['filename'] = new_filename\n",
    "            \n",
    "            balanced_dfs.append(sample)\n",
    "            duplicates_added += 1\n",
    "    else:\n",
    "        print(f\"\\n{disease}: {n_samples} (already >= target, keeping all)\")\n",
    "        # Keep only up to target to avoid over-representation\n",
    "        balanced_dfs.append(group.iloc[:TARGET_SAMPLES_PER_CLASS])\n",
    "\n",
    "# Combine all balanced data\n",
    "df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\n‚úì Dataset balanced!\")\n",
    "print(f\"  Total samples: {len(df_balanced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Balanced Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_counts = df_balanced['disease'].value_counts()\n",
    "\n",
    "print(\"\\nFinal disease distribution (after balancing):\")\n",
    "for disease in sorted(balanced_counts.index):\n",
    "    count = balanced_counts[disease]\n",
    "    print(f\"  {disease}: {count} samples\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x=sorted(balanced_counts.index), y=[balanced_counts[d] for d in sorted(balanced_counts.index)])\n",
    "plt.axhline(y=TARGET_SAMPLES_PER_CLASS, color='g', linestyle='--', label=f'Target: {TARGET_SAMPLES_PER_CLASS}')\n",
    "plt.title('Disease Distribution (After Balancing)')\n",
    "plt.xlabel('Disease')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create Disease-to-Index Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create disease to index mapping (sorted alphabetically for consistency)\n",
    "diseases = sorted(df_balanced['disease'].unique())\n",
    "disease2idx = {disease: i for i, disease in enumerate(diseases)}\n",
    "\n",
    "print(f\"Disease to Index Mapping ({len(disease2idx)} classes):\")\n",
    "for disease, idx in disease2idx.items():\n",
    "    count = len(df_balanced[df_balanced['disease'] == disease])\n",
    "    print(f\"  {idx}: {disease} ({count} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Create Output Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DISEASE_IMAGES_TRAIN.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DISEASE_IMAGES_VAL.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DISEASE_LABELS_TRAIN.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DISEASE_LABELS_VAL.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Output directories created\")\n",
    "print(f\"  {OUTPUT_DISEASE_BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Convert to YOLO Format and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox_to_yolo(row):\n",
    "    x_center = (row['xmin'] + row['xmax']) / 2 / row['width']\n",
    "    y_center = (row['ymin'] + row['ymax']) / 2 / row['height']\n",
    "    bbox_width = (row['xmax'] - row['xmin']) / row['width']\n",
    "    bbox_height = (row['ymax'] - row['ymin']) / row['height']\n",
    "    return x_center, y_center, bbox_width, bbox_height\n",
    "\n",
    "def export_to_yolo(df, images_dir, output_images_dir, output_labels_dir, class_mapping):\n",
    "    exported = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    for filename, group in df.groupby(\"filename\"):\n",
    "        try:\n",
    "            # Check if this is a duplicate (has _dup in name)\n",
    "            if '_dup' in filename:\n",
    "                # Get original filename\n",
    "                original_filename = filename.split('_dup')[0] + Path(filename).suffix\n",
    "                src = Path(images_dir) / original_filename\n",
    "            else:\n",
    "                src = Path(images_dir) / filename\n",
    "            \n",
    "            if not src.exists():\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            dst = Path(output_images_dir) / filename\n",
    "            shutil.copy2(src, dst)\n",
    "            \n",
    "            # Create label file\n",
    "            label_file = Path(output_labels_dir) / (Path(filename).stem + \".txt\")\n",
    "            with open(label_file, \"w\") as f:\n",
    "                for _, row in group.iterrows():\n",
    "                    cls_idx = class_mapping[row['disease']]\n",
    "                    x_c, y_c, w, h = convert_bbox_to_yolo(row)\n",
    "                    f.write(f\"{cls_idx} {x_c:.6f} {y_c:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "            exported += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {filename} - {e}\")\n",
    "            skipped += 1\n",
    "    \n",
    "    return exported, skipped\n",
    "\n",
    "print(\"Exporting to YOLO format...\")\n",
    "exported, skipped = export_to_yolo(\n",
    "    df_balanced, \n",
    "    TRAIN_IMAGES_DIR, \n",
    "    OUTPUT_DISEASE_IMAGES_TRAIN, \n",
    "    OUTPUT_DISEASE_LABELS_TRAIN,\n",
    "    disease2idx\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Export complete!\")\n",
    "print(f\"  Exported: {exported} images\")\n",
    "print(f\"  Skipped: {skipped} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Generate YAML Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_content = {\n",
    "    'path': str(OUTPUT_DISEASE_BASE_DIR.resolve()),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'nc': len(disease2idx),\n",
    "    'names': {idx: disease for disease, idx in disease2idx.items()}\n",
    "}\n",
    "\n",
    "yaml_path = OUTPUT_DISEASE_BASE_DIR / 'dataset.yaml'\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"‚úì YAML configuration created\")\n",
    "print(f\"\\nLocation: {yaml_path}\")\n",
    "print(f\"\\nClasses ({len(disease2idx)}):\")\n",
    "for idx in sorted(yaml_content['names'].keys()):\n",
    "    print(f\"  {idx}: {yaml_content['names'][idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DISEASE CLASSIFICATION DATASET READY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Total samples: {len(df_balanced)}\")\n",
    "print(f\"  Number of disease classes: {len(disease2idx)}\")\n",
    "print(f\"  Target per class: {TARGET_SAMPLES_PER_CLASS}\")\n",
    "print(f\"\\n  Class distribution:\")\n",
    "for disease in sorted(disease2idx.keys()):\n",
    "    count = len(df_balanced[df_balanced['disease'] == disease])\n",
    "    print(f\"    [{disease2idx[disease]}] {disease}: {count} samples\")\n",
    "print(f\"\\nüìÅ Location: {OUTPUT_DISEASE_BASE_DIR}\")\n",
    "print(f\"üìù Config: {yaml_path}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Important: This model should only process samples\")\n",
    "print(f\"   classified as 'disease' by the binary model (Pipeline 1)\")\n",
    "print(f\"\\n‚úÖ Ready for YOLO training!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
